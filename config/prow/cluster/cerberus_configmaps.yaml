---
kind: ConfigMap
apiVersion: v1
metadata:
  name: cerberus-config
  namespace: prow
data:
  kubernetes_config.yaml: |
    cerberus:
        distribution: kubernetes                             # Distribution can be kubernetes or openshift
        kubeconfig_path: /root/.kube/config                      # Path to kubeconfig
        watch_nodes: True                                    # Set to True for the cerberus to monitor the cluster nodes
        watch_cluster_operators: False                       # Set to True for cerberus to monitor cluster operators. Enable it only when distribution is openshift
        watch_url_routes:                                    # Route url's you want to monitor, this is a double array with the url and optional authorization parameter
        cerberus_publish_status: True                        # When enabled, cerberus starts a light weight http server and publishes the status
        inspect_components: False                            # Enable it only when OpenShift client is supported to run
                                                             # When enabled, cerberus collects logs, events and metrics of failed components

        prometheus_url:                                      # The prometheus url/route is automatically obtained in case of OpenShift, please set it when the distribution is Kubernetes.
        prometheus_bearer_token:                             # The bearer token is automatically obtained in case of OpenShift, please set it when the distribution is Kubernetes. This is needed to authenticate with prometheus.
                                                             # This enables Cerberus to query prometheus and alert on observing high Kube API Server latencies.

        slack_integration: True                              # When enabled, cerberus reports the failed iterations in the slack channel
                                                             # The following env vars needs to be set: SLACK_API_TOKEN ( Bot User OAuth Access Token ) and SLACK_CHANNEL ( channel to send notifications in case of failures )
                                                             # When slack_integration is enabled, a watcher can be assigned for each day. The watcher of the day is tagged while reporting failures in the slack channel. Values are slack member ID's.
        watcher_slack_ID:                                        # (NOTE: Defining the watcher id's is optional and when the watcher slack id's are not defined, the slack_team_alias tag is used if it is set else no tag is used while reporting failures in the slack channel.)
            Monday:
            Tuesday:
            Wednesday:
            Thursday:
            Friday:
            Saturday:
            Sunday:
        slack_team_alias: prow-job-alert                         # The slack team alias to be tagged while reporting failures in the slack channel when no watcher is assigned
        custom_checks:
            -   custom_checks/check_pod_controller_health.py

    tunings:
        iterations: 5                                        # Iterations to loop before stopping the watch, it will be replaced with infinity when the daemon mode is enabled
        sleep_time: 60                                       # Sleep duration between each iteration
        daemon_mode: True                                    # Iterations are set to infinity which means that the cerberus will monitor the resources forever

    database:
        database_path: /tmp/cerberus.db                      # Path where cerberus database needs to be stored
        reuse_database: False                                # When enabled, the database is reused to store the failures
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: cerberus-custom-check
  namespace: prow
data:
  check_pod_controller_health.py: |
    import logging
    import cerberus.invoke.command as runcommand


    def check_name():
        logging.info("Check health of prow controller\n")

    def check():
        pod_name =  runcommand.invoke("kubectl -n prow get pods -o custom-columns=\":metadata.name\" | grep controller")
        pod_log = runcommand.invoke("kubectl -n prow logs %s" % (pod_name))

        if ((pod_log.find("Failed to get API Group-Resources")) != -1):
            message = "Prow controller Pod has Error"
            logging.info(message)
            return False, message
        else:
            message = "Prow controller Pod has No Error"
            logging.info(message)
            return True, message

    def main():
        check_name()
        status, message = check()
        return {'status':status, 'message':message}
